{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 - Web-scraping**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial length of authors list: 1524\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ic2s2-2023.org/program\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#get content in section with id \"main\"\n",
    "main = soup.find_all('section', id='main')\n",
    "\n",
    "#get the list of each <\\i> tag in the main section\n",
    "authors_list = []\n",
    "\n",
    "for section in main:\n",
    "    #find italic tags since authors are in italic\n",
    "    items = section.find_all('i')\n",
    "    for item in items:\n",
    "        authors = item.text\n",
    "        for author in authors.split(\",\"):\n",
    "            author_strip = author.strip()\n",
    "            #remove Chair: in the beginning of the name if it exists\n",
    "            if author_strip.startswith(\"Chair:\"):\n",
    "                author_strip = author_strip[6:]\n",
    "            authors_list.append(author_strip)\n",
    "\n",
    "#remove duplicates\n",
    "authors_list = list(set(authors_list))\n",
    "print(f\"initial length of authors list: {len(authors_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use fuzzywuzzy to remove similar authors to further prevent misspelling and duplicates\n",
    "def remove_similar_authors(authors, similarity_threshold = 85):\n",
    "    unique_authors = []\n",
    "    for author in authors:\n",
    "        # Check if the author is too similar to any already in unique_authors\n",
    "        if not unique_authors:\n",
    "            unique_authors.append(author)\n",
    "        else:\n",
    "            similarities = [process.extractOne(author, [ua])[1] for ua in unique_authors]\n",
    "            if all(sim < similarity_threshold for sim in similarities):\n",
    "                unique_authors.append(author)\n",
    "    return unique_authors\n",
    "\n",
    "# #sort authors list\n",
    "# authors_list.sort()\n",
    "\n",
    "# #split authors list into lists of n authors to speed up the process\n",
    "# n = 100\n",
    "# split_authors = [authors_list[i:i + n] for i in range(0, len(authors_list), n)]\n",
    "# cleaned_authors = []\n",
    "# for split in split_authors:\n",
    "#     cleaned_authors += remove_similar_authors(split)\n",
    "\n",
    "# print(f\"length of cleaned authors: {len(cleaned_authors)}\")\n",
    "# # Save the list to a file with UTF-8 encoding to handle special characters\n",
    "# with open(\"authors.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for author in cleaned_authors:\n",
    "#         f.write(author + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many researchers did you get?\\\n",
    "Initially, we retrieved 1524 authors, After cleaning the list of authors, we retrieved 1394 authors\n",
    "\n",
    "2. Explain the process you followed to web-scrape the page \\\n",
    "For scraping, we noticed authors names were italicized so we retrieved this text. This also contained people with the Chair: tag in front, so we removed tag this by a simple comparison while retaining the name in the set. After this we used fuzzywuzzy to extract and compare similar author names. This removes duplicates as a result of misspellings or alternative names and such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 - Ready Made vs Custom Made Data**\n",
    "1. What are the pros and cons \\\n",
    "...\n",
    "2. How can these differences influence interpretation (max 150. words)\\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 - Gathering Research Articles using the OpenAlex API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import concurrent.futures\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load author txt\n",
    "def load_author_txt(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        authors = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if len(line) > 0:\n",
    "                authors.append(line)\n",
    "    return authors\n",
    "\n",
    "authors = load_author_txt('authors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1237 authors from authors_df.csv\n"
     ]
    }
   ],
   "source": [
    "endpoint = \"https://api.openalex.org/authors?search=\"\n",
    "\n",
    "\n",
    "def get_author_response(author):\n",
    "    url = endpoint + author\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#helper function to get the concepts from the author dictionary and the intercection if both required concept groups are present\n",
    "def get_concepts(author_dict):\n",
    "    concepts = [[c.get('display_name') for c in author_dict.get('results')[a].get('x_concepts')] for a in range(len(author_dict.get('results')))]\n",
    "    return concepts\n",
    "\n",
    "def get_best_match(concepts):\n",
    "    target_1 = ['Sociology', 'Psychology', 'Economics', 'Political Science']\n",
    "    target_2 = ['Mathematics', 'Physics', 'Computer Science']\n",
    "\n",
    "    for i, author in enumerate(concepts):\n",
    "        #get authors with concepts in target_1 AND target_2\n",
    "        if (x in author for x in target_1) and (x in author for x in target_2):\n",
    "            return i\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def get_info(auth_info):\n",
    "    concepts = get_concepts(auth_info)\n",
    "    c_index = get_best_match(concepts)\n",
    "    \n",
    "    #get their id\n",
    "    author_id = auth_info.get('results')[c_index].get('id')[-11:]\n",
    "    #get their display_name\n",
    "    display_name = auth_info.get('results')[c_index].get('display_name')\n",
    "    #get their works_api_url\n",
    "    works_api_url = \"https://api.openalex.org/works?filter=author.id:\" + author_id\n",
    "    #get their h_index\n",
    "    h_index = auth_info.get('results')[c_index].get('summary_stats').get('h_index')\n",
    "    #get their works count\n",
    "    works_count = auth_info.get('results')[c_index].get('works_count')\n",
    "    #get their cited_by_count\n",
    "    cited_by_count = auth_info.get('results')[c_index].get('cited_by_count')\n",
    "    \n",
    "    #get their country code\n",
    "    country_code = auth_info.get('results')[c_index].get('last_known_institution').get('country_code')\n",
    "\n",
    "    return {\n",
    "        'author_id': author_id,\n",
    "        'display_name': display_name,\n",
    "        'works_api_url': works_api_url,\n",
    "        'h_index': h_index,\n",
    "        'works_count': works_count,\n",
    "        'cited_by_count': cited_by_count,\n",
    "        'country_code': country_code\n",
    "    }\n",
    "\n",
    "def get_author_df(authors,*, n_workers = 3, load_if_exists = True, save_path = 'authors_df.csv'):\n",
    "    \n",
    "    if load_if_exists:\n",
    "        try:\n",
    "            authors_df = pd.read_csv(save_path)\n",
    "            print(f'Loaded {len(authors_df)} authors from {save_path}')\n",
    "            return authors_df\n",
    "        except:\n",
    "            print(f'Failed to load {save_path}, will create a new one..')\n",
    "            \n",
    "    authors_df = pd.DataFrame(columns=['author_id', 'display_name', 'works_api_url', 'h_index', 'works_count', 'cited_by_count', 'country_code'])\n",
    "    err_authors = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        future_to_author = {executor.submit(get_author_response, author): author for author in authors}\n",
    "        futures = tqdm.tqdm(concurrent.futures.as_completed(future_to_author), total=len(authors))\n",
    "        for future in futures:\n",
    "            author = future_to_author[future]\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    try:\n",
    "                        info = get_info(res)\n",
    "                        authors_df = pd.concat([authors_df, pd.DataFrame([info])])\n",
    "                    except:\n",
    "                        err_authors.append(author)\n",
    "                else:\n",
    "                    err_authors.append(author)\n",
    "            except Exception as e:\n",
    "                err_authors.append(author)\n",
    "    print(f'Done!, Failed authors: {len(err_authors)}')\n",
    "    #save to csv\n",
    "\n",
    "    authors_df.to_csv(save_path, index=False)\n",
    "    return authors_df\n",
    "\n",
    "authors_df = get_author_df(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 - The Network of Computational Social Scientists**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
