{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Lucas Sylvester - s214636 <br>\n",
    "Lucas Emcken - s214625 <br>\n",
    "Lila <br>\n",
    "**Github** \\\n",
    "https://github.com/LucasEmcken/SocialScienceAssignments\n",
    "\n",
    "**Contribution statement** \\\n",
    "Everybody made all exercises before hand.\\\n",
    "The final assignment was made by combining and \\\n",
    "discussing different approaches to the each exercise.\\\n",
    "\\\n",
    "The git history is not representative as it was made using\\\n",
    " VS code Live Share to work in unison on one computer in real time.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Web-scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial length of authors list: 1524\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ic2s2-2023.org/program\"\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#get content in section with id \"main\"\n",
    "main = soup.find_all('section', id='main')\n",
    "\n",
    "#get the list of each <\\i> tag in the main section\n",
    "authors_list = []\n",
    "\n",
    "for section in main:\n",
    "    #find italic tags since authors are in italic\n",
    "    items = section.find_all('i')\n",
    "    for item in items:\n",
    "        authors = item.text\n",
    "        for author in authors.split(\",\"):\n",
    "            author_strip = author.strip()\n",
    "            #remove Chair: in the beginning of the name if it exists\n",
    "            if author_strip.startswith(\"Chair:\"):\n",
    "                author_strip = author_strip[6:]\n",
    "            authors_list.append(author_strip)\n",
    "\n",
    "#remove duplicates\n",
    "authors_list = list(set(authors_list))\n",
    "print(f\"initial length of authors list: {len(authors_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use fuzzywuzzy to remove similar authors to further prevent misspelling and duplicates\n",
    "def remove_similar_authors(authors, similarity_threshold = 85):\n",
    "    unique_authors = []\n",
    "    for author in authors:\n",
    "        # Check if the author is too similar to any already in unique_authors\n",
    "        if not unique_authors:\n",
    "            unique_authors.append(author)\n",
    "        else:\n",
    "            similarities = [process.extractOne(author, [ua])[1] for ua in unique_authors]\n",
    "            if all(sim < similarity_threshold for sim in similarities):\n",
    "                unique_authors.append(author)\n",
    "    return unique_authors\n",
    "\n",
    "# #sort authors list\n",
    "# authors_list.sort()\n",
    "\n",
    "# #split authors list into lists of n authors to speed up the process\n",
    "# n = 100\n",
    "# split_authors = [authors_list[i:i + n] for i in range(0, len(authors_list), n)]\n",
    "# cleaned_authors = []\n",
    "# for split in split_authors:\n",
    "#     cleaned_authors += remove_similar_authors(split)\n",
    "\n",
    "# print(f\"length of cleaned authors: {len(cleaned_authors)}\")\n",
    "# # Save the list to a file with UTF-8 encoding to handle special characters\n",
    "# with open(\"authors.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for author in cleaned_authors:\n",
    "#         f.write(author + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How many researchers did you get?**\\\n",
    "Initially, we retrieved 1524 authors.\n",
    "By sortin the list and cleaning using a Levenshtein distance similarity measure we got 1394 authors.\n",
    "The threshhold for if names where the same where put at 85, all the cases we observed with this threshhold semeed to be correct. It might have missed a few really significand misspellings, but as in some of the examples below, it will have cleaned the majority of misspellings and variations.\n",
    "\n",
    "**2. Explain the process you followed to web-scrape the page** \\\n",
    "For scraping, we noticed authors names were italicized so we retrieved this text. This also contained people with the Chair: tag in front, so we removed tag this by a simple comparison while retaining the name in the set. After this we used fuzzywuzzy to extract and compare similar author names. This removes duplicates as a result of misspellings or alternative names and such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of similar names removed:** \\\n",
    "Alessandro Flamini \\\n",
    "Alessandro Flammini \\\n",
    "similarity:97 \\\n",
    "\\\n",
    "Alexander Gates \\\n",
    "Alexander J Gates \\\n",
    "similarity:94 \\\n",
    "\\\n",
    "Ana Maria Jaramillo \\\n",
    "Ana MarÃ­a Jaramillo \\\n",
    "similarity:95\\\n",
    "\\\n",
    "Anne C Kroon\\\n",
    "Anne C. Kroon\\\n",
    "similarity: 96\\\n",
    "\\\n",
    "Anne C Kroon\\\n",
    "Anne Kroon\\\n",
    "similarity:91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Ready Made vs Custom Made Data\n",
    "**1. What are the pros and cons** \\\n",
    "...\n",
    "**2. How can these differences influence interpretation (max 150. words)**\\\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "import concurrent.futures\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load author txt\n",
    "def load_author_txt(file_path):\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        authors = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if len(line) > 0:\n",
    "                authors.append(line)\n",
    "    return authors\n",
    "\n",
    "authors = load_author_txt('authors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1237 authors from authors_df.csv\n"
     ]
    }
   ],
   "source": [
    "endpoint = \"https://api.openalex.org/authors?search=\"\n",
    "\n",
    "\n",
    "def get_author_response(author):\n",
    "    url = endpoint + author\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#helper function to get the concepts from the author dictionary and the intercection if both required concept groups are present\n",
    "def get_concepts(author_dict):\n",
    "    concepts = [[c.get('display_name') for c in author_dict.get('results')[a].get('x_concepts')] for a in range(len(author_dict.get('results')))]\n",
    "    return concepts\n",
    "\n",
    "def get_best_match(concepts):\n",
    "    target_1 = ['Sociology', 'Psychology', 'Economics', 'Political Science']\n",
    "    target_2 = ['Mathematics', 'Physics', 'Computer Science']\n",
    "\n",
    "    for i, author in enumerate(concepts):\n",
    "        #get authors with concepts in target_1 AND target_2\n",
    "        if (x in author for x in target_1) and (x in author for x in target_2):\n",
    "            return i\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def get_info(auth_info):\n",
    "    concepts = get_concepts(auth_info)\n",
    "    c_index = get_best_match(concepts)\n",
    "    \n",
    "    #get their id\n",
    "    author_id = auth_info.get('results')[c_index].get('id')[-11:]\n",
    "    #get their display_name\n",
    "    display_name = auth_info.get('results')[c_index].get('display_name')\n",
    "    #get their works_api_url\n",
    "    works_api_url = \"https://api.openalex.org/works?filter=author.id:\" + author_id\n",
    "    #get their h_index\n",
    "    h_index = auth_info.get('results')[c_index].get('summary_stats').get('h_index')\n",
    "    #get their works count\n",
    "    works_count = auth_info.get('results')[c_index].get('works_count')\n",
    "    #get their cited_by_count\n",
    "    cited_by_count = auth_info.get('results')[c_index].get('cited_by_count')\n",
    "    \n",
    "    #get their country code\n",
    "    country_code = auth_info.get('results')[c_index].get('last_known_institution').get('country_code')\n",
    "\n",
    "    return {\n",
    "        'author_id': author_id,\n",
    "        'display_name': display_name,\n",
    "        'works_api_url': works_api_url,\n",
    "        'h_index': h_index,\n",
    "        'works_count': works_count,\n",
    "        'cited_by_count': cited_by_count,\n",
    "        'country_code': country_code\n",
    "    }\n",
    "\n",
    "def get_author_df(authors,*, n_workers = 3, load_if_exists = True, save_path = 'authors_df.csv'):\n",
    "    \n",
    "    if load_if_exists:\n",
    "        try:\n",
    "            authors_df = pd.read_csv(save_path)\n",
    "            print(f'Loaded {len(authors_df)} authors from {save_path}')\n",
    "            return authors_df\n",
    "        except:\n",
    "            print(f'Failed to load {save_path}, will create a new one..')\n",
    "            \n",
    "    authors_df = pd.DataFrame(columns=['author_id', 'display_name', 'works_api_url', 'h_index', 'works_count', 'cited_by_count', 'country_code'])\n",
    "    err_authors = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        future_to_author = {executor.submit(get_author_response, author): author for author in authors}\n",
    "        futures = tqdm.tqdm(concurrent.futures.as_completed(future_to_author), total=len(authors))\n",
    "        for future in futures:\n",
    "            author = future_to_author[future]\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    try:\n",
    "                        info = get_info(res)\n",
    "                        authors_df = pd.concat([authors_df, pd.DataFrame([info])])\n",
    "                    except:\n",
    "                        err_authors.append(author)\n",
    "                else:\n",
    "                    err_authors.append(author)\n",
    "            except Exception as e:\n",
    "                err_authors.append(author)\n",
    "    print(f'Done!, Failed authors: {len(err_authors)}')\n",
    "    #save to csv\n",
    "\n",
    "    authors_df.to_csv(save_path, index=False)\n",
    "    return authors_df\n",
    "\n",
    "authors_df = get_author_df(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_authors = authors_df[(authors_df['works_count'] > 5) & (authors_df['works_count'] < 5000) &\n",
    "                              (authors_df['cited_by_count'] > 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "works found, formatting..\n",
      "Loaded 30287 works from combined_works.csv\n"
     ]
    }
   ],
   "source": [
    "def get_intercept(concepts):\n",
    "    target = ['Sociology', 'Psychology', 'Economics', 'Political Science']\n",
    "    \n",
    "    if any(x in concepts for x in target):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_works(url):\n",
    "    \n",
    "    # response = requests.get(url)\n",
    "    filters = 'concepts_count:%3E1,authors_count:%3C10,concept.id:C33923547|C121332964|c41008148&per-page=200'\n",
    "    filter_url = url + filters\n",
    "    # return response.json()\n",
    "    response = requests.get(filter_url, timeout=10).json()\n",
    "    return_dict = {\n",
    "        'id': [],\n",
    "        'publication_year': [],\n",
    "        'cited_by_count': [],\n",
    "        'author_ids': [],\n",
    "        'title': [],\n",
    "        'abstract_inverted_index': []\n",
    "    }\n",
    "    \n",
    "    page = 1\n",
    "    # print(len(response.get('results')))\n",
    "    while len(response.get('results')) != 0:\n",
    "        for paper in response.get('results'):\n",
    "            concepts = [c['display_name'] for c in paper['concepts']]\n",
    "            if get_intercept(concepts):\n",
    "                # return_dict['id'] = paper['id']\n",
    "                return_dict['id'].append(paper['id'])\n",
    "                return_dict['publication_year'].append(paper['publication_year'])\n",
    "                return_dict['cited_by_count'].append(paper['cited_by_count'])\n",
    "                \n",
    "                #authorships\n",
    "                return_dict['author_ids'].append([id.get('author').get('id') for id in paper['authorships']])\n",
    "                \n",
    "                \n",
    "                return_dict['title'].append(paper['title'])\n",
    "                # return_dict['abstract_inverted_index'] = ' '.join(list(paper['abstract_inverted_index'].keys())) if paper['abstract_inverted_index'] else []\n",
    "                return_dict['abstract_inverted_index'].append(paper['abstract_inverted_index'])\n",
    "        \n",
    "        page += 1\n",
    "        \n",
    "        filters = f'concepts_count:%3E1,authors_count:%3C10,concept.id:C33923547|C121332964|c41008148&per-page=200&page={page}'\n",
    "        filter_url = url + filters\n",
    "        response = requests.get(filter_url).json()\n",
    "                \n",
    "    return return_dict\n",
    "    \n",
    "\n",
    "def combine_works(df, n_workers=3, load_if_exists=True, save_path='combined_works.csv'):\n",
    "    if load_if_exists:\n",
    "        try:\n",
    "            combined_df = pd.read_csv(save_path)\n",
    "            print('works found, formatting..')\n",
    "            combined_df = combined_df.dropna()\n",
    "            combined_df['author_ids'] = combined_df['author_ids'].apply(lambda x: ast.literal_eval(x))\n",
    "            combined_df['abstract_inverted_index'] = combined_df['abstract_inverted_index'].apply(ast.literal_eval)\n",
    "            print(f'Loaded {len(combined_df)} works from {save_path}')\n",
    "            return combined_df\n",
    "        except:\n",
    "            print(f'Failed to load {save_path}, will create a new one..')\n",
    "    \n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    failed = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        results = list(tqdm.tqdm(executor.map(get_works, df['works_api_url']), total=len(df)))\n",
    "        for i in range(len(results)):\n",
    "            try:\n",
    "                works_df = pd.DataFrame(results[i])\n",
    "                combined_df = pd.concat([combined_df, works_df])\n",
    "            except:\n",
    "                failed.append(i)\n",
    "    \n",
    "    print(f'Done!, Failed authors: {len(failed)}')\n",
    "    #save to csv\n",
    "    combined_df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "full_df = combine_works(pruned_authors, n_workers=5, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9394\n"
     ]
    }
   ],
   "source": [
    "full_df = full_df[full_df['cited_by_count'] > 10]\n",
    "full_df = full_df[full_df['author_ids'].apply(lambda x: len(x) < 10)]\n",
    "\n",
    "#split into IC2S2 papers and IC2S2 abstracts\n",
    "ic2s2_papers = full_df[['id', 'publication_year', 'cited_by_count', 'author_ids']]\n",
    "ic2s2_abstracts = full_df[['id', 'title', 'abstract_inverted_index']]\n",
    "\n",
    "print(len(ic2s2_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 989 authors from co_authors.csv\n"
     ]
    }
   ],
   "source": [
    "#get the authors of the papers\n",
    "def get_paper_authors(paper):\n",
    "    author_ids = []\n",
    "    for ids in paper['author_ids']:\n",
    "        author_ids += ids\n",
    "    author_ids = list(set(author_ids))\n",
    "    \n",
    "    api_authors = []\n",
    "    for id in author_ids:\n",
    "        api_authors.append(f'https://api.openalex.org/authors/{id[-11:]}')\n",
    "    \n",
    "    return api_authors\n",
    "\n",
    "co_authors_apis = get_paper_authors(ic2s2_papers)\n",
    "\n",
    "def get_author_info(url):\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10).json()\n",
    "        return_dict = {\n",
    "            'id': \"https://openalex.org/\" + url[-11:],\n",
    "            'display_name': response.get('display_name'),\n",
    "            'works_api_url': response.get('works_api_url'),\n",
    "            'h_index': response.get('summary_stats').get('h_index'),\n",
    "            'works_count': response.get('works_count'),\n",
    "            'country_code': response.get('last_known_institution').get('country_code')\n",
    "        }\n",
    "        return return_dict\n",
    "    except:\n",
    "        return_dict = {\n",
    "            'display_name': None,\n",
    "            'works_api_url': None,\n",
    "            'h_index': None,\n",
    "            'works_count': None,\n",
    "            'country_code': None\n",
    "        }\n",
    "    \n",
    "        return return_dict\n",
    "\n",
    "def combine_authors(author_urls, n_workers=3, load_if_exists=True, save_path='co_authors.csv'):\n",
    "    if load_if_exists:\n",
    "        try:\n",
    "            combined_df = pd.read_csv(save_path)\n",
    "            combined_df = combined_df.dropna()\n",
    "            print(f'Loaded {len(combined_df)} authors from {save_path}')\n",
    "            return combined_df\n",
    "        except:\n",
    "            print(f'Failed to load {save_path}, will create a new one..')\n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        results = list(tqdm.tqdm(executor.map(get_author_info, author_urls), total=len(author_urls)))\n",
    "        for i in range(len(results)):\n",
    "            author_df = pd.DataFrame(results[i], index=[0])\n",
    "            combined_df = pd.concat([combined_df, author_df])\n",
    "    #save to csv\n",
    "    combined_df = combined_df.dropna()\n",
    "    combined_df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "co_authors_df = combine_authors(co_authors_apis[:1000], load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\AppData\\Local\\Temp\\ipykernel_13700\\127905344.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  authorship_df = pd.concat([authorship_df, pd.DataFrame({'paper_id': paper['id'], 'author_ids': [author_ids], 'display_names': [display_names], 'countries': [countries], 'citation_count': [paper['cited_by_count']], 'publication_year': [paper['publication_year']]})])\n"
     ]
    }
   ],
   "source": [
    "#create an authorship df with the display names from the co_authors_df compared with the author_ids from the ic2s2_papers\n",
    "authorship_df = pd.DataFrame(columns=['paper_id', 'author_ids', 'display_names', 'countries', 'citation_count', 'publication_year'])\n",
    "\n",
    "for i in range(len(ic2s2_papers)):\n",
    "    paper = ic2s2_papers.iloc[i]\n",
    "    author_ids = paper['author_ids']\n",
    "    display_names = []\n",
    "    countries = []\n",
    "    for id in author_ids:\n",
    "        author = co_authors_df[co_authors_df['id'] == id]\n",
    "        if len(author) > 0:\n",
    "            display_names.append(author['display_name'].values[0])\n",
    "            countries.append(author['country_code'].values[0])\n",
    "    authorship_df = pd.concat([authorship_df, pd.DataFrame({'paper_id': paper['id'], 'author_ids': [author_ids], 'display_names': [display_names], 'countries': [countries], 'citation_count': [paper['cited_by_count']], 'publication_year': [paper['publication_year']]})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by length of display names\n",
    "authorship_df['len_display_names'] = authorship_df['display_names'].apply(lambda x: len(x))\n",
    "authorship_df = authorship_df.sort_values(by='len_display_names', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset summary.** \\\n",
    "**How many works are listed in your IC2S2 papers dataframe?**\\\n",
    "**How many unique researchers have co-authored these works?**\\\n",
    "**Efficiency in code.**\\\n",
    "**Describe the strategies you implemented to make your code more efficient.** \\\n",
    "We implementet the following strategies:\n",
    "1. Filtering directly in query to return less more relevant results, instead of filtering in code.\n",
    "2. Multithreading\n",
    "3. \n",
    "**How did your approach affect your code's execution time? (answer in max 150 words)**\\\n",
    "Significantly?\n",
    "**Filtering Criteria and Dataset Relevance** \\\n",
    "**Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? (answer in max 150 words)**\\\n",
    "\n",
    "The threshholding can be used to filter off mistakes. The api is not perfect.\\ and ther instances of duplicates of authors, or authors who have several thousand works, aliases etc. Its for instance reasonable to that authors with more than 5000 works, might have some mistakes in their record or similar.\n",
    "\\\n",
    "The limit of 10 authors per work, can be used to at least keep\n",
    " the extension of the author list relevant. If there are more than 10 authors for a work the majority of them might have had very little part in work and are therefore probaly not relevant. The authors that are relevant, would likely show up on other works and be included through there. Likewise with the minimum citation of 10, it resonable to say that established relevant work have at least 10 citations.\n",
    "\n",
    "However, brand new reasearch that just been published might be under represented. It might be that a new paper has not yet been reviewed and cited by other papers. Likewise older papers, that are less relevant, might eventually get 10 citations even though its less relevant than newer papers.\n",
    "\n",
    "\\\n",
    "\n",
    "\n",
    "\n",
    "**Sociology OR Psychology OR Economics OR Political Science AND Mathematics OR Physics OR Computer Science**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a weighted edgelist weighted edgelist where each list element is a tuple containing three elements: the author ids of two collaborating authors and the total number of papers they've co-authored. Ensure each author pair is listed only once.\n",
    "\n",
    "weighted_edgelist = []\n",
    "\n",
    "def weighted_creation (df): \n",
    "\n",
    "    #list of all author    \n",
    "    auth_pairs = []\n",
    "    \n",
    "    #get the list of authors in the dataframe \n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        authors = row['author_ids']\n",
    "    \n",
    "        for pair in combinations(authors, 2): \n",
    "            \n",
    "            auth_pairs.append(tuple(sorted(pair))) \n",
    "\n",
    "    #Count the occurrences of each author pair to determine the number of co-authored papers\n",
    "    \n",
    "    edge_weights = Counter(auth_pairs)\n",
    "\n",
    "    # Convert the edge weights to a list of tuples in the format (author_id_1, author_id_2, weight)\n",
    "    weighted_edgelist = [(pair[0], pair[1], weight) for pair, weight in edge_weights.items()]\n",
    "\n",
    "    return weighted_edgelist\n",
    "\n",
    "weighted_creation(authorship_df)\n",
    "\n",
    "def graph_construction (weighted_edgelist):\n",
    "    # Create a new graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    G.add_weighted_edges_from(weighted_edgelist)\n",
    "    return G\n",
    "\n",
    "def node_attributes(G, df, file_path): \n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        \n",
    "        # Check if the author is a node in the graph\n",
    "        if G.has_node(row['author_ids']):\n",
    "            # Set the author's attributes\n",
    "            nx.set_node_attributes(G, {row['author_id']: {\n",
    "                'display_name': row['display_name'],\n",
    "                'country': row['country'],\n",
    "                'citation_count': row['citation_count'],\n",
    "                'year_first_publication': row['year_first_publication']\n",
    "            }})\n",
    "\n",
    "    graph_data = nx.node_link_data(G)\n",
    "    \n",
    "    # Write the graph data to a JSON file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(graph_data, f)\n",
    "    print(f\"Network graph saved as {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network graph saved as network_graph.json\n"
     ]
    }
   ],
   "source": [
    "weighted_edgelist = weighted_creation(authorship_df)\n",
    "graph = graph_construction(weighted_edgelist)\n",
    "node_attributes(graph, authorship_df, 'network_graph.json')\n",
    "\n",
    "# #plot the graph\n",
    "# def plot_graph(G, file_path):\n",
    "#     plt.figure(figsize=(20, 20))\n",
    "#     pos = nx.spring_layout(G, k=0.05)\n",
    "#     nx.draw(G, pos, node_size=10, width=0.1)\n",
    "#     plt.savefig(file_path)\n",
    "#     print(f\"Network graph saved as {file_path}\")\n",
    "\n",
    "# plot_graph(graph, 'network_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary Network Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Network metrics\n",
      "Total number( of nodes: 12676, total number of links: 39559\n",
      "Network density: 0.0004924306786092839, The network is sparse, since only 0.05% of all possible links are present.\n",
      "The network is connected: False\n",
      "The network has 235 connected components.\n",
      "The size of the largest connected component: 9818\n",
      "The networks has 0 isolated nodes.\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Network metrics\")\n",
    "print(f\"Total number( of nodes: {len(graph.nodes)}, total number of links: {len(graph.edges)}\")\n",
    "density = nx.density(graph)\n",
    "print(f\"Network density: {density}, The network is sparse, since only {np.round((density)*100, 2)}% of all possible links are present.\")\n",
    "print(f\"The network is connected: {nx.is_connected(graph)}\")\n",
    "print(f\"The network has {nx.number_connected_components(graph)} connected components.\")\n",
    "print(f\"The size of the largest connected component: {len(max(nx.connected_components(graph), key=len))}\")\n",
    "print(f\"The networks has {len(list(nx.isolates(graph)))} isolated nodes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Degree analysis\n",
      "Average degree: 6.24, median degree: 5.0, minimum degree: 1, maximum degree: 184\n",
      "3. Top authors\n",
      "https://openalex.org/A5088141761: 184\n",
      "https://openalex.org/A5044944954: 155\n",
      "https://openalex.org/A5075080019: 151\n",
      "https://openalex.org/A5029100305: 138\n",
      "https://openalex.org/A5017914184: 135\n"
     ]
    }
   ],
   "source": [
    "print(f\"2. Degree analysis\")\n",
    "avg_degree = np.mean(list(dict(graph.degree()).values()))\n",
    "avg_degree = np.round(avg_degree, 2)\n",
    "median_degree = np.median(list(dict(graph.degree()).values()))\n",
    "minimum_degree = min(dict(graph.degree()).values())\n",
    "maximum_degree = max(dict(graph.degree()).values())\n",
    "print(f\"Average degree: {avg_degree}, median degree: {median_degree}, minimum degree: {minimum_degree}, maximum degree: {maximum_degree}\")\n",
    "\n",
    "print(\"3. Top authors\")\n",
    "top_authors = sorted(dict(graph.degree()).items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for author, degree in top_authors[:5]:\n",
    "    print(f\"{author}: {degree}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What area do the authors specialize in? (max 150 words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
